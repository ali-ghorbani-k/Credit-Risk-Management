{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xp4I20KBm54b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-089c46bdb1fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# Classification metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Import deep learning (tensorflow.keras) and relevant pyhthon libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.decomposition import PCA \n",
    "from lightgbm import LGBMClassifier\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "# Ignore ConvergenceWarning messages\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnfSFoq6PlE2"
   },
   "outputs": [],
   "source": [
    "def train_application():\n",
    "        \n",
    "    \"\"\"\n",
    "    This function reads application_train.csv and application_test.csv\n",
    "    files, cleans them and perform manual feature engineering for each\n",
    "    application (SK_ID_CURR).   \n",
    "    \n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    train, test: two pandas.DataFrame which includes hand engineered features from \n",
    "                  just application_train and application_test tables.   \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Processing application_train and application_test tables')\n",
    "    train = pd.read_csv('application_train.csv')\n",
    "    # Delete four applications with XNA CODE_GENDER (train set)\n",
    "    train = train[train['CODE_GENDER'] != 'XNA']\n",
    "    # Replace DAYS_EMPLOYED = 365243 by nan\n",
    "    train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True) \n",
    "    # Feature engineering\n",
    "    train['Days_employed_age']   = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\n",
    "    train['Credit_income_ratio'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "    train['Anuity_income_ratio'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL'] \n",
    "    train['Income_per_person']   = train['AMT_INCOME_TOTAL'] / train['CNT_FAM_MEMBERS']\n",
    "    #length of the payment in months since the annuity is the monthly amount due\n",
    "    train['Credit_term'] = train['AMT_ANNUITY']/train['AMT_CREDIT'] \n",
    "    # testset\n",
    "    test = pd.read_csv('application_test.csv')\n",
    "    test['Days_employed_age']   = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\n",
    "    test['Credit_income_ratio'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "    test['Anuity_income_ratio'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL'] \n",
    "    test['Income_per_person']   = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']\n",
    "    test['Credit_term']         = test['AMT_ANNUITY'] / test['AMT_CREDIT'] \n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyilwZIvUekt"
   },
   "outputs": [],
   "source": [
    "def bureau_CNN_features(train, test, nrows):\n",
    "    ''' \n",
    "    Convolution Nueral Network (CNN) is used to extract new feture from sequential historical data,\n",
    "    Customer journey information based on Bureau table.\n",
    "    \n",
    "    Parameters:\n",
    "    train: preprocessed training data \n",
    "    test: preprocessed test data\n",
    "    nrows: number of rows considered in train data for the model due to the computational power limitation.\n",
    "\n",
    "    Returns:\n",
    "    train, test: train and test dataframes with added the new features from applying CNN on the bureau and bureau_balance tables  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print('Extracting features using Convolutional Neural Network (CNN) ...')\n",
    "    train = train[:nrows]\n",
    "    idl = train['SK_ID_CURR'].values\n",
    "    bureau = pd.read_csv('bureau.csv')\n",
    "    # Imputating the missing data in bureau table\n",
    "    # Missing categorical features are imputed with 'Not_applicable'\n",
    "    # Missing numeric features are imputed with Zero (logical choice for this dataset)\n",
    "    cols = bureau.select_dtypes(include = object).columns\n",
    "    bureau [cols] = bureau[cols].fillna('Not_Applicable')\n",
    "    cols = bureau.select_dtypes(exclude = object).columns\n",
    "    bureau[cols] = bureau[cols].fillna(0)\n",
    "    # One-hot encoding of categorical features\n",
    "    bureau = pd.get_dummies(bureau, drop_first= True)\n",
    "    bureau = bureau.sort_values('DAYS_CREDIT', ascending= False)\n",
    "    lst = bureau['SK_ID_CURR'].values\n",
    "    lst = list(set(lst))\n",
    "    lst.sort()\n",
    "    \n",
    "    # Making bureau table data structure similar to an image\n",
    "    # Applications are grouoped by SK_ID_CURR and for each SK_ID_CURR, the 5 most recent SK_ID_BUREAU is considered.\n",
    "    # If an SK_ID_CURR did not have 5 records, empty rows were added and filled with -99 (to avoid confusion with zero).  \n",
    "    \n",
    "    group = bureau.groupby('SK_ID_CURR')\n",
    "    b = [] # b is the reshaped data structure of bureau table, suitable for use in CNN\n",
    "    j=0\n",
    "    for sk in idl:\n",
    "        if sk in lst:\n",
    "            a = group.get_group(lst[j])\n",
    "            if a.shape[0] >= 5:\n",
    "                a = a[:5]\n",
    "            else:\n",
    "                # m99 represents rows having value of -99\n",
    "                m99 = np.ones((5-a.shape[0],a.shape[1]))*-99\n",
    "                m99 = pd.DataFrame(m99, columns=a.columns)\n",
    "                a   = a.append(m99)\n",
    "            a = a.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], axis=1)    \n",
    "            a = a.values.flatten().tolist()\n",
    "            b.extend(a)\n",
    "            j += 1\n",
    "        else:\n",
    "            m99 = np.ones((5,bureau.shape[1]))*-99\n",
    "            m99 = pd.DataFrame(m99, columns=bureau.columns)\n",
    "            m99 = m99.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], axis=1)    \n",
    "            m99 = m99.values.flatten().tolist()\n",
    "            b.extend(m99)\n",
    "    b = np.array(b)\n",
    "    b = np.reshape(b,(idl.shape[0], 5, bureau.shape[1]-2, 1))\n",
    "    print('shape of channel(bureau):', b.shape)\n",
    "    y = train['TARGET']\n",
    "    y = to_categorical(y,2)     \n",
    "    \n",
    "    # Deep CNN implementation\n",
    "    # CNN architecture includes 2 convoution layer followed by two fully connected layer\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "    np.random.seed(5)        \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #1st conv layer\n",
    "    model.add(Conv2D(filters = 32, kernel_size= (5,5), padding=\"same\",\n",
    "                    input_shape=(b.shape[1],b.shape[2],1),data_format=\"channels_last\"\n",
    "                      ))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    #2nd conv layer\n",
    "    model.add(Conv2D(32, (5,5), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #FC1\n",
    "    model.add(Dense(units= 128))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #FC2\n",
    "    model.add(Dense(units= 100, name= 'feature_extract'))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    #output FC\n",
    "    model.add(Dense(units= 2, activation='sigmoid'))\n",
    "    model.build()\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    model.summary()\n",
    "\n",
    "    # Train deep neural network\n",
    "    early_stops = EarlyStopping(patience=5, monitor='val_auc')\n",
    "    mc = ModelCheckpoint('best_model.h5',\n",
    "                      monitor='val_loss', \n",
    "                      verbose=0, \n",
    "                      save_best_only=True)\n",
    "    model.fit(b, y, validation_split=0.05, callbacks=[early_stops, mc], batch_size= batch_size, epochs= epochs, verbose=1)\n",
    "    \n",
    "    # Extract the useful featuer from CNN after training the deep nerual network\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('feature_extract').output)\n",
    "    intermediate_layer_model.summary()\n",
    "    \n",
    "    #predict to get featured data\n",
    "    feauture_engg_data = intermediate_layer_model.predict(b)\n",
    "    feauture_engg_data = pd.DataFrame(feauture_engg_data)\n",
    "    print('feauture_engg_data shape:', feauture_engg_data.shape)\n",
    "    \n",
    "    # Renaming columns\n",
    "    new_col = []\n",
    "    for i in range(100):\n",
    "        new_col.append('bfeat_%d'%(i+1))\n",
    "    feauture_engg_data.columns = new_col\n",
    "    feauture_engg_data['SK_ID_CURR'] = idl\n",
    "    feauture_engg_data.to_csv('100_feature.csv', index = False)\n",
    "    feauture_engg_data.head(5)  #The features are unnamed now\n",
    "\n",
    "    # test set cnn features extraction\n",
    "    idl_test = test['SK_ID_CURR'].values\n",
    "    b_test = []\n",
    "    j=0\n",
    "    for sk in idl_test:\n",
    "        if sk in lst:\n",
    "            a = group.get_group(lst[j])\n",
    "            if a.shape[0] >= 5:\n",
    "                a = a[:5]\n",
    "            else:\n",
    "                m99 = np.ones((5-a.shape[0],a.shape[1]))*-99\n",
    "                m99 = pd.DataFrame(m99, columns=a.columns)\n",
    "                a   = a.append(m99)\n",
    "            a = a.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], axis=1)    \n",
    "            a = a.values.flatten().tolist()\n",
    "            b_test.extend(a)\n",
    "            j += 1\n",
    "        else:\n",
    "            m99 = np.ones((5,bureau.shape[1]))*-99\n",
    "            m99 = pd.DataFrame(m99, columns=bureau.columns)\n",
    "            m99 = m99.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], axis=1)    \n",
    "            m99 = m99.values.flatten().tolist()\n",
    "            b_test.extend(m99)\n",
    "    b_test = np.array(b_test)\n",
    "    b_test = np.reshape(b_test,(idl_test.shape[0], 5, bureau.shape[1]-2, 1))\n",
    "    \n",
    "    #predict to get featured data\n",
    "    feauture_engg_data_test = intermediate_layer_model.predict(b_test)\n",
    "    feauture_engg_data_test = pd.DataFrame(feauture_engg_data_test)\n",
    "    \n",
    "    # Renaming columns\n",
    "    new_col = []\n",
    "    for i in range(100):\n",
    "        new_col.append('bfeat_%d'%(i+1))\n",
    "    feauture_engg_data_test.columns = new_col\n",
    "    feauture_engg_data_test['SK_ID_CURR'] = idl_test\n",
    "    # Merge CNN features to train and test datasets\n",
    "    train = train.merge(feauture_engg_data, on='SK_ID_CURR', how='left')\n",
    "    test  = test.merge(feauture_engg_data_test, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJrpJG-_YJEv"
   },
   "outputs": [],
   "source": [
    "def preprocessing(train, test):\n",
    "    '''\n",
    "    This function calculates the correlation between all features in training data and drops the columns with \n",
    "    correlation > 0.98.\n",
    "\n",
    "    Parameters:\n",
    "    train: train set\n",
    "\n",
    "    Returns:\n",
    "    train: pandas.DataFrame which includes preprocessed training data.\n",
    "    \n",
    "    '''\n",
    "    print('Preprocessing final table and label encoding categorical features...')\n",
    "    # Drop the columns with correlation > -0.98\n",
    "    corr = train.corr()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\n",
    "    train = train.drop(to_drop, axis=1)\n",
    "    test = test.drop(to_drop, axis=1)\n",
    "    # Encoding categorical features because lightGBM offers good accuracy with integer-encoded categorical features. \n",
    "    class_le = LabelEncoder()\n",
    "    cols = train.select_dtypes(include= object).columns\n",
    "    for col in cols:\n",
    "        train[col] = class_le.fit_transform(train[col].values.astype(str))\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col]  = class_le.fit_transform(test[col].values.astype(str))\n",
    "        test[col]  = test[col].astype('category')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58siGCS0gBdw"
   },
   "outputs": [],
   "source": [
    "def lightGBM(train, test, num_folds, test_size):\n",
    "    \n",
    "    '''\n",
    "    This function trains a machine learning model using LightGBM algorithm. \n",
    "\n",
    "    Parameters:\n",
    "    train, test: preprocessed dataframes with added CNN features\n",
    "    num_folds: number of folds for cross-validation (default is 5)\n",
    "    test_size: ratio of test to train in the training dataset\n",
    "\n",
    "    Returns:\n",
    "    pred_class: Binary class prediction of the target variable.\n",
    "    pred: Probability prediction of the target variable.\n",
    "    y_test: y_test in the trainig dataset\n",
    "    '''\n",
    "    print('Applying LightGBM algorithm...')\n",
    "    y = train['TARGET']\n",
    "    X = train.drop('TARGET', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = test_size, random_state=1234)\n",
    "    # Lighgbm parameters was found using Bayesian Optimization\n",
    "    model_params = {\n",
    "      'colsample_bytree': 0.45544541538547634,\n",
    "      'learning_rate': 0.09712737568777673,\n",
    "      'max_depth': 10,\n",
    "      'min_child_weight': 44.81416318834993,\n",
    "      'min_split_gain': 0.47913323843650946,\n",
    "      'num_leaves': 44,\n",
    "      'reg_alpha': 8.507126649843658,\n",
    "      'reg_lambda': 2.2113739093853257,\n",
    "      'subsample': 0.43342993037373423\n",
    "    }\n",
    "    model = make_pipeline(StandardScaler(), LGBMClassifier(**model_params))\n",
    "    # cross validation scores\n",
    "    scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=num_folds, n_jobs=-1, verbose = 100)\n",
    "    print('max cross_val AUC: ', np.max(scores))\n",
    "    model.fit(X_train, y_train)\n",
    "    # Binary class prediction\n",
    "    pred_class = model.predict(X_test)\n",
    "    # Probability prediction\n",
    "    pred = model.predict_proba(X_test)\n",
    "    pred = [p[1] for p in pred]\n",
    "    # Model prediction for Kaggle  \n",
    "    lgbmpred = model.predict_proba(test)\n",
    "    submit = test[['SK_ID_CURR']].copy()\n",
    "    submit['TARGET'] = lgbmpred[:,1]\n",
    "    submit.to_csv('lgbmkaggle.csv', index= False)\n",
    "    return pred_class, pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7c4nGwKgwk8"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(pred_class, pred, y_test):\n",
    "    \n",
    "    '''\n",
    "    This function calculates the classificaiton metrics including precision, recall, F1-Score, AUC_ROC, and cohen-kappa coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    pred_class: Binary class prediction of the target variable.\n",
    "    pred: Probability prediction of the target variable.\n",
    "    y_test: y_test in the trainig dataset\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    \n",
    "    # ROC_AUC score\n",
    "    print('ROC_AUC:', roc_auc_score(y_test, pred))\n",
    "    # Precision/Recall (0.1 Threshold)\n",
    "    pred_class_2 = (np.array(pred) > 0.1).astype(int)\n",
    "    cm = confusion_matrix(y_test, pred_class_2)\n",
    "    print('\\nConfusion_metrix (0.1 Threshold): \\n', cm)\n",
    "    # True Negatives (TN)\n",
    "    tn = cm[0][0]\n",
    "    # False Positives (FP)\n",
    "    fp = cm[0][1]    \n",
    "    # False Negatives (FN)\n",
    "    fn = cm[1][0]\n",
    "    # True Positives (TP)\n",
    "    tp = cm[1][1]\n",
    "    precision = tp / (tp + fp)\n",
    "    print( 'Precision (0.1 Threshold): ', precision )\n",
    "    recall = tp / (tp + fn)\n",
    "    print( 'Recall (0.1 Threshold): ', recall )\n",
    "    print( 'F1-score ( 0.1 Threshold):', 2*precision*recall/(precision+recall))\n",
    "    cohen_kappa = cohen_kappa_score(y_test, pred_class_2)\n",
    "    print( '\\nCohen_kappa (0.1 Threshold): ', cohen_kappa )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8ybPq4kmqY5"
   },
   "outputs": [],
   "source": [
    "def plot_ROC(y_test, pred):\n",
    "    '''\n",
    "    This function plots ROC based on y_test and predictied probability of positive class by lightGBM.\n",
    "\n",
    "    Parameters:\n",
    "    pred: Probability prediction of the target variable.\n",
    "    y_test: y_test in the trainig dataset\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    # Initialize figure\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "    plt.plot(fpr, tpr)\n",
    "    # Diagonal 45 degree line\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    # Axes limits and labels\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTA4FJTygyPM"
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_test, pred):\n",
    "    '''\n",
    "    This function plots precicision_recall curve based on y_test and predictied probability of positive class by lightGBM..\n",
    "\n",
    "    Parameters:\n",
    "    pred: Probability prediction of the target variable.\n",
    "    y_test: y_test in the trainig dataset\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, pred)\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    plt.title('Precision_Recall')\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision)\n",
    "    # Axes limits and labels\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 659458,
     "status": "ok",
     "timestamp": 1598049601257,
     "user": {
      "displayName": "Ali Ghorbani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWTYg2QaNPZd4GGNiDkkHX8r9t7BRVHYGz3JwSKA=s64",
      "userId": "10869472433171243113"
     },
     "user_tz": 240
    },
    "id": "Oz97j9-8LGX2",
    "outputId": "7eb8a0f9-b4ab-4c96-88a5-d4a52b6df1c1"
   },
   "outputs": [],
   "source": [
    "''' -------------------------------Main---------------------------\n",
    "The main calls all funcitons sequentillay, merge the tables, train lightGBM, and plot the classification metrics.\n",
    "\n",
    "LGBM performance is compared in two following considitons: \n",
    "    1) training data with concatenated CNN features \n",
    "    2) training data without CNN features.\n",
    "    \n",
    "'''\n",
    "# nrows: number of rows considered in train data for the model due to the computational power limitation.\n",
    "nrows = 30000\n",
    "\n",
    "# 1) Model performance with concatenating CNN features\n",
    "train, test = train_application()\n",
    "# Extract new features using CNN\n",
    "train, test = bureau_CNN_features(train, test, nrows)\n",
    "# Preprocesing including label encoding of categorical varibles for LightGBM \n",
    "train, test = preprocessing(train, test)\n",
    "# Training LightGBM\n",
    "nfolds = 5 # number of folds for cross-validation (default is 5)\n",
    "test_size = 0.05 # ratio of test to train dataset \n",
    "pred_class, pred, y_test = lightGBM(train,test, nfolds, test_size)\n",
    "# Evalutate ROC_AUC, Precision, Recall, F1-Score, Kohen-Cappa metrics\n",
    "calculate_metrics(pred_class, pred, y_test)\n",
    "# Plot ROC curve\n",
    "plot_ROC(y_test, pred)\n",
    "# Plot Precision/R curve\n",
    "plot_precision_recall(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 33851,
     "status": "ok",
     "timestamp": 1598047654709,
     "user": {
      "displayName": "Ali Ghorbani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWTYg2QaNPZd4GGNiDkkHX8r9t7BRVHYGz3JwSKA=s64",
      "userId": "10869472433171243113"
     },
     "user_tz": 240
    },
    "id": "e_ixaPuhuQMz",
    "outputId": "b3f45733-8b9a-4c4c-9e2f-c4ca14eae183"
   },
   "outputs": [],
   "source": [
    "# 2) Model performance without CNN features\n",
    "train, test = train_application()\n",
    "train= train[:nrows]\n",
    "# Preprocesing including label encoding of categorical varibles for LightGBM \n",
    "train, test = preprocessing(train, test)\n",
    "train = train[:nrows]\n",
    "# Training LightGBM\n",
    "nfolds = 5\n",
    "test_size = 0.05\n",
    "pred_class, pred, y_test = lightGBM(train,test, nfolds, test_size)\n",
    "# Evalutate ROC_AUC, Precision, Recall, F1-Score, Kohen-Cappa metrics\n",
    "calculate_metrics(pred_class, pred, y_test)\n",
    "# Plot ROC curve\n",
    "plot_ROC(y_test, pred)\n",
    "# Plot Precision/R curve\n",
    "plot_precision_recall(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LGBM_CNN Feature Extraction.ipynb",
   "provenance": [
    {
     "file_id": "1FJJk5YUyHB7zSjt3He5YrQ0OI4gQ_zMR",
     "timestamp": 1598026406827
    },
    {
     "file_id": "1R5IYeiRZufzRipTqBk5zcdMgdkmUoEHE",
     "timestamp": 1596279690660
    },
    {
     "file_id": "1ZnpsttZrvD6wbKD6aETZuuMbHnE7FLFi",
     "timestamp": 1594932112784
    },
    {
     "file_id": "1QIbvwXBVKCtoJHH1alQcP13irUjezFYp",
     "timestamp": 1594841666146
    },
    {
     "file_id": "1W2dSR-Ua7USBX0gTUcVxEtEvVlEbZCsq",
     "timestamp": 1594734735941
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
